{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b04735f5-f20d-4ed4-8157-8a73dada1ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add metadata when merging\n",
    "\n",
    "### FROM PANAROO MERGE_NODES.PY\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "#from .isvalid import del_dups\n",
    "import numpy as np\n",
    "#from intbitset import intbitset\n",
    "\n",
    "\n",
    "def gen_node_iterables(G, nodes, feature, split=None):\n",
    "    for n in nodes:\n",
    "        if split is None:\n",
    "            yield G.nodes[n][feature]\n",
    "        else:\n",
    "            yield G.nodes[n][feature].split(split)\n",
    "\n",
    "\n",
    "def gen_edge_iterables(G, edges, feature):\n",
    "    for e in edges:\n",
    "        yield G[e[0]][e[1]][feature]\n",
    "\n",
    "\n",
    "def temp_iter(list_list):\n",
    "    for n in list_list:\n",
    "        yield n\n",
    "\n",
    "\n",
    "def iter_del_dups(iterable):\n",
    "    seen = {}\n",
    "    for f in itertools.chain.from_iterable(iterable):\n",
    "        seen[f] = None\n",
    "    return (list(seen.keys()))\n",
    "\n",
    "\n",
    "def del_dups(iterable):\n",
    "    seen = {}\n",
    "    for f in iterable:\n",
    "        seen[f] = None\n",
    "    return (list(seen.keys()))\n",
    "\n",
    "\n",
    "def merge_node_cluster(G,\n",
    "                       nodes,\n",
    "                       newNode,\n",
    "                       multi_centroid=True,\n",
    "                       check_merge_mems=True):\n",
    "\n",
    "    if check_merge_mems:\n",
    "        mem_count = Counter(\n",
    "            itertools.chain.from_iterable(\n",
    "                gen_node_iterables(G, nodes, 'members')))\n",
    "        # genome IDs can be identical since they're being merged, so comment out this bit\n",
    "        #if max(mem_count.values()) > 1:\n",
    "            #raise ValueError(\"merging nodes with the same genome IDs!\")\n",
    "\n",
    "    # take node with most support as the 'consensus'\n",
    "    nodes = sorted(nodes, key=lambda x: G.nodes[x]['size'])\n",
    "\n",
    "    # First create a new node and combine the attributes\n",
    "    dna = iter_del_dups(gen_node_iterables(G, nodes, 'dna'))\n",
    "    maxLenId = 0\n",
    "    max_l = 0\n",
    "    for i, s in enumerate(dna):\n",
    "        if len(s) >= max_l:\n",
    "            max_l = len(s)\n",
    "            maxLenId = i\n",
    "            \n",
    "    members = G.nodes[nodes[0]]['members'].copy()\n",
    "    for n in nodes[1:]:\n",
    "        members = list(dict.fromkeys(G.nodes[n]['members']))\n",
    "\n",
    "    if multi_centroid:\n",
    "        mergedDNA = any(gen_node_iterables(G, nodes, 'mergedDNA'))\n",
    "    else:\n",
    "        mergedDNA = True\n",
    "\n",
    "    G.add_node(\n",
    "        newNode,\n",
    "        size=len(members),\n",
    "        centroid=iter_del_dups(gen_node_iterables(G, nodes, 'centroid')),\n",
    "        maxLenId=maxLenId,\n",
    "        members=members,\n",
    "        seqIDs=set(iter_del_dups(gen_node_iterables(G, nodes, 'seqIDs'))),\n",
    "        #seqIDs=set(set(gen_node_iterables(G, nodes, 'seqIDs')) | set(gen_node_iterables(G, newNode, 'seqIDs'))),\n",
    "        hasEnd=any(gen_node_iterables(G, nodes, 'hasEnd')),\n",
    "        protein=iter_del_dups(gen_node_iterables(G, nodes, 'protein')),\n",
    "        dna=dna,\n",
    "        #geneIDs=\";\".join(iter_del_dups(gen_node_iterables(G, nodes, 'geneIDs', split=\";\"))),  # <-- added geneID merge for uGIDs\n",
    "        annotation=\";\".join(\n",
    "            iter_del_dups(gen_node_iterables(G, nodes, 'annotation',\n",
    "                                             split=\";\"))),\n",
    "        description=\";\".join(\n",
    "            iter_del_dups(\n",
    "                gen_node_iterables(G, nodes, 'description', split=\";\"))),\n",
    "        lengths=list(\n",
    "            itertools.chain.from_iterable(\n",
    "                gen_node_iterables(G, nodes, 'lengths'))),\n",
    "        longCentroidID=max(gen_node_iterables(G, nodes, 'longCentroidID')),\n",
    "        paralog=any(gen_node_iterables(G, nodes, 'paralog')),\n",
    "        mergedDNA=mergedDNA)\n",
    "    if \"prevCentroids\" in G.nodes[nodes[0]]:\n",
    "        G.nodes[newNode]['prevCentroids'] = \";\".join(\n",
    "            set(\n",
    "                iter_del_dups(\n",
    "                    gen_node_iterables(G, nodes, 'prevCentroids', split=\";\"))))\n",
    "\n",
    "    # Now iterate through neighbours of each node and add them to the new node\n",
    "    merge_nodes = set(nodes)\n",
    "    for node in nodes:\n",
    "        for neighbour in G.neighbors(node):\n",
    "            if neighbour in merge_nodes: continue\n",
    "            if G.has_edge(newNode, neighbour):\n",
    "                G[newNode][neighbour]['members'] = list(dict.fromkeys(G[node][neighbour][\n",
    "                    'members']))\n",
    "                G[newNode][neighbour]['size'] = len(G[newNode][neighbour]['members'])\n",
    "            else:\n",
    "                G.add_edge(newNode,\n",
    "                           neighbour,\n",
    "                           size=G[node][neighbour]['size'],\n",
    "                           members=G[node][neighbour]['members'])\n",
    "\n",
    "    # remove old nodes from Graph\n",
    "    G.remove_nodes_from(nodes)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def delete_node(G, node):\n",
    "    # add in new edges\n",
    "    for mem in G.nodes[node]['members']:\n",
    "        mem_edges = list(\n",
    "            set([e[1] for e in G.edges(node) if mem in G.edges[e]['members']]))\n",
    "        if len(mem_edges) < 2: continue\n",
    "        for n1, n2 in itertools.combinations(mem_edges, 2):\n",
    "            if G.has_edge(n1, n2):\n",
    "                G[n1][n2]['members'] |= intbitset([mem])\n",
    "                G[n1][n2]['size'] = len(G[n1][n2]['members'])\n",
    "            else:\n",
    "                G.add_edge(n1, n2, size=1, members=intbitset([mem]))\n",
    "\n",
    "    # now remove node\n",
    "    G.remove_node(node)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def remove_member_from_node(G, node, member):\n",
    "\n",
    "    # add in replacement edges if required\n",
    "    mem_edges = list(\n",
    "        set([e[1] for e in G.edges(node) if member in G.edges[e]['members']]))\n",
    "    if len(mem_edges) > 1:\n",
    "        for n1, n2 in itertools.combinations(mem_edges, 2):\n",
    "            if G.has_edge(n1, n2):\n",
    "                G[n1][n2]['members'] |= intbitset([member])\n",
    "                G[n1][n2]['size'] = len(G[n1][n2]['members'])\n",
    "            else:\n",
    "                G.add_edge(n1, n2, size=1, members=intbitset([member]))\n",
    "\n",
    "    # remove member from node\n",
    "    G.nodes[node]['members'].discard(member)\n",
    "    G.nodes[node]['seqIDs'] = set([\n",
    "        sid for sid in G.nodes[node]['seqIDs']\n",
    "        if sid.split(\"_\")[0] != str(member)\n",
    "    ])\n",
    "    G.nodes[node]['size'] -= 1\n",
    "\n",
    "    # remove member from edges of node\n",
    "    edges_to_remove = []\n",
    "    for e in G.edges(node):\n",
    "        if member in G.edges[e]['members']:\n",
    "            if len(G.edges[e]['members']) == 1:\n",
    "                edges_to_remove.append(e)\n",
    "            else:\n",
    "                G.edges[e]['members'].discard(member)\n",
    "                G.edges[e]['size'] = len(G.edges[e]['members'])\n",
    "    for e in edges_to_remove:\n",
    "        G.remove_edge(*e)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ca615-08a8-463c-9e00-df7b55c62ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot merged graph\n",
    "\n",
    "#merged_graph = nx.read_gml(\"/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_merged/merged_graph.gml\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "nx.draw(merged_graph, with_labels=False, node_color='red', edge_color='gray', node_size = 2)\n",
    "plt.show()\n",
    "\n",
    "# plot joint graph\n",
    "\n",
    "graph_all = nx.read_gml(\"/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_all/final_graph.gml\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "nx.draw(graph_all, with_labels=False, node_color='red', edge_color='gray', node_size = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac8b3b-8c64-42d5-85c4-d996195eb538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_graphs(graph_files, n_cpu=1):\n",
    "    for graph_file in graph_files:\n",
    "        if not os.path.isfile(graph_file):\n",
    "            print(\"Missing:\", graph_file)\n",
    "            raise RuntimeError(\"Missing graph file!\")\n",
    "\n",
    "    graphs = [nx.read_gml(graph_file) for graph_file in graph_files]\n",
    "    isolate_names = list(\n",
    "        itertools.chain.from_iterable(\n",
    "            [G.graph['isolateNames'] for G in graphs]))\n",
    "\n",
    "    member_count = 0\n",
    "    node_count = 0\n",
    "    id_mapping = []\n",
    "    for i, G in enumerate(graphs):\n",
    "        id_mapping.append({})\n",
    "        # relabel nodes to be consecutive integers from 1\n",
    "        mapping = {}\n",
    "        for n in G.nodes():\n",
    "            mapping[n] = node_count\n",
    "            node_count += 1\n",
    "        G = nx.relabel_nodes(G, mapping, copy=True)\n",
    "\n",
    "        # set up edge members and remove conflicts.\n",
    "        #for e in G.edges():\n",
    "        #    G[e[0]][e[1]]['members'] = [\n",
    "        #        m + member_count for m in conv_list(G[e[0]][e[1]]['members'])\n",
    "        #    ]\n",
    "\n",
    "        # set up node parameters and remove conflicts.\n",
    "        max_mem = -1\n",
    "        for n in G.nodes():\n",
    "            ncentroids = []\n",
    "            for sid in G.nodes[n]['centroid'].split(\";\"):\n",
    "                nid = update_sid(sid, member_count)\n",
    "                id_mapping[i][sid] = nid\n",
    "                if \"refound\" not in nid:\n",
    "                    ncentroids.append(nid)\n",
    "            G.nodes[n]['centroid'] = ncentroids\n",
    "            new_ids = set()\n",
    "            for sid in conv_list(G.nodes[n]['seqIDs']):\n",
    "                nid = update_sid(sid, member_count)\n",
    "                id_mapping[i][sid] = nid\n",
    "                new_ids.add(nid)\n",
    "            G.nodes[n]['seqIDs'] = new_ids\n",
    "            G.nodes[n]['protein'] = del_dups(G.nodes[n]['protein'].replace(\n",
    "                '*', 'J').split(\";\"))\n",
    "            G.nodes[n]['dna'] = del_dups(G.nodes[n]['dna'].split(\";\"))\n",
    "            G.nodes[n]['lengths'] = conv_list(G.nodes[n]['lengths'])\n",
    "            G.nodes[n]['longCentroidID'][1] = update_sid(\n",
    "                G.nodes[n]['longCentroidID'][1], member_count)\n",
    "            G.nodes[n]['members'] = [m + member_count for m in conv_list(G.nodes[n]['members'])]\n",
    "            max_mem = max(max_mem, max(G.nodes[n]['members']))\n",
    "\n",
    "        member_count = max_mem + 1\n",
    "        graphs[i] = G\n",
    "\n",
    "    return graphs, isolate_names, id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2c12c-9032-4dad-bf0c-9f71e0d1b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context search\n",
    "\n",
    "G.neighbors() # from nx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
