{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c25facf-514f-44c3-8bd1-72b4b07e19cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import networkx as nx\n",
    "import scipy as scipy\n",
    "import matplotlib.pyplot as plt # for visualizing GMLs\n",
    "import os\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as sklearn_metrics\n",
    "from sklearn.metrics import rand_score,mutual_info_score,adjusted_rand_score,adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b613b-a624-44c4-820c-a7de6bc6f8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping Clusters of Orthologous Genes (COGs) between two pan-genome gene graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "844278b4-4724-4f70-beaa-0088f69eee72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define functions to read in graphs with metadata\n",
    "\n",
    "def conv_list(maybe_list):\n",
    "    if not isinstance(maybe_list, list):\n",
    "        maybe_list = [maybe_list]\n",
    "    return (maybe_list)\n",
    "\n",
    "def update_sid(sid, member_count):\n",
    "    sid = sid.split(\"_\")\n",
    "    sid[0] = str(member_count + int(sid[0].replace(\"'\", \"\")))\n",
    "    return (\"_\".join(sid))\n",
    "\n",
    "def del_dups(iterable):\n",
    "    seen = {}\n",
    "    for f in iterable:\n",
    "        seen[f] = None\n",
    "    return (list(seen.keys()))\n",
    "\n",
    "def load_graphs(graph_files, n_cpu=1):\n",
    "    for graph_file in graph_files:\n",
    "        if not os.path.isfile(graph_file):\n",
    "            print(\"Missing:\", graph_file)\n",
    "            raise RuntimeError(\"Missing graph file!\")\n",
    "\n",
    "    graphs = [nx.read_gml(graph_file) for graph_file in graph_files]\n",
    "    isolate_names = list(\n",
    "        itertools.chain.from_iterable(\n",
    "            [G.graph['isolateNames'] for G in graphs]))\n",
    "\n",
    "    member_count = 0\n",
    "    node_count = 0\n",
    "    id_mapping = []\n",
    "    for i, G in enumerate(graphs):\n",
    "        id_mapping.append({})\n",
    "        # relabel nodes to be consecutive integers from 1\n",
    "        mapping = {}\n",
    "        for n in G.nodes():\n",
    "            mapping[n] = node_count\n",
    "            node_count += 1\n",
    "        G = nx.relabel_nodes(G, mapping, copy=True)\n",
    "\n",
    "        # set up edge members and remove conflicts.\n",
    "        for e in G.edges():\n",
    "            G[e[0]][e[1]]['members'] = [\n",
    "                m + member_count for m in conv_list(G[e[0]][e[1]]['members'])\n",
    "            ]\n",
    "\n",
    "        # set up node parameters and remove conflicts.\n",
    "        max_mem = -1\n",
    "        for n in G.nodes():\n",
    "            ncentroids = []\n",
    "            for sid in G.nodes[n]['centroid'].split(\";\"):\n",
    "                nid = update_sid(sid, member_count)\n",
    "                id_mapping[i][sid] = nid\n",
    "                if \"refound\" not in nid:\n",
    "                    ncentroids.append(nid)\n",
    "            G.nodes[n]['centroid'] = ncentroids\n",
    "            new_ids = set()\n",
    "            for sid in conv_list(G.nodes[n]['seqIDs']):\n",
    "                nid = update_sid(sid, member_count)\n",
    "                id_mapping[i][sid] = nid\n",
    "                new_ids.add(nid)\n",
    "            G.nodes[n]['seqIDs'] = new_ids\n",
    "            G.nodes[n]['protein'] = del_dups(G.nodes[n]['protein'].replace(\n",
    "                '*', 'J').split(\";\"))\n",
    "            G.nodes[n]['dna'] = del_dups(G.nodes[n]['dna'].split(\";\"))\n",
    "            G.nodes[n]['lengths'] = conv_list(G.nodes[n]['lengths'])\n",
    "            G.nodes[n]['longCentroidID'][1] = update_sid(\n",
    "                G.nodes[n]['longCentroidID'][1], member_count)\n",
    "            G.nodes[n]['members'] = [m + member_count for m in conv_list(G.nodes[n]['members'])]\n",
    "            max_mem = max(max_mem, max(G.nodes[n]['members']))\n",
    "\n",
    "        member_count = max_mem + 1\n",
    "        graphs[i] = G\n",
    "\n",
    "    return graphs, isolate_names, id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af11d73-85e7-4875-80a6-28748f1d0194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### read in two graphs\n",
    "\n",
    "graph_files = [\"/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_1/final_graph.gml\", \"/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_2/final_graph.gml\"]\n",
    "\n",
    "graphs, isolate_names, id_mapping = load_graphs(graph_files)\n",
    "\n",
    "sim_graph_04 = graphs[0]\n",
    "sim_graph_59 = graphs[1]\n",
    "\n",
    "#list(sim_graph_59.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c594146-001c-4559-8b99-7a079247d187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_data_all = pd.read_csv('/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_all/gene_data.csv')\n",
    "gene_data_g1 = pd.read_csv('/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_1/gene_data.csv')\n",
    "gene_data_g2 = pd.read_csv('/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_2/gene_data.csv')\n",
    "\n",
    "# check annotation_ids for uniqueness (to ensure that this is a suitable ID for identifying the genes with)\n",
    "#combined_annotations = set(gene_data_g1['annotation_id']).union(set(gene_data_g2['annotation_id']))\n",
    "#gene_annotations = set(gene_data_all['annotation_id'])\n",
    "#annotations_match = combined_annotations == gene_annotations\n",
    "\n",
    "#print(f\"annotation_ids match: {annotations_match}\")\n",
    "\n",
    "#print(f\"duplicated IDs: {gene_data_g1['annotation_id'].duplicated().any()}\")\n",
    "#print(f\"duplicated IDs: {gene_data_g2['annotation_id'].duplicated().any()}\")\n",
    "#print(f\"duplicated IDs: {gene_data_all['annotation_id'].duplicated().any()}\")\n",
    "\n",
    "# match geneIDs using annotation_id data\n",
    "\n",
    "# rename column\n",
    "gene_data_all = gene_data_all.rename(columns={'clustering_id': 'clustering_id_all'})\n",
    "gene_data_g1 = gene_data_g1.rename(columns={'clustering_id': 'clustering_id_indiv'})\n",
    "gene_data_g2 = gene_data_g2.rename(columns={'clustering_id': 'clustering_id_indiv'})\n",
    "\n",
    "# match clustering_ids from overall run to clustering_ids from individual runs using annotation_ids\n",
    "\n",
    "# first match by annotation ids:\n",
    "matches_g1 = gene_data_all[['annotation_id', 'clustering_id_all']].merge(\n",
    "    gene_data_g1[['annotation_id', 'clustering_id_indiv']],\n",
    "    on='annotation_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "matches_g2 = gene_data_all[['annotation_id', 'clustering_id_all']].merge(\n",
    "    gene_data_g2[['annotation_id', 'clustering_id_indiv']],\n",
    "    on='annotation_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#print(matches_g1)\n",
    "\n",
    "#print(matches_g1.loc[matches_g1['clustering_id_indiv'] == '43_2_209', 'clustering_id_all'].values[0])\n",
    "\n",
    "# then replace individual run GID with GID from run of all data\n",
    "def indGID_to_allGID(G, matches):\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        node_GIDs = G.nodes[node].get(\"geneIDs\", \"error\")\n",
    "        #print(f\"node_GIDs: {node_GIDs}\")\n",
    "        \n",
    "        node_GIDs_split = \";\".join(str(matches.loc[matches['clustering_id_indiv'] == str(GID), 'clustering_id_all'].values[0]) for GID in node_GIDs.split(\";\"))\n",
    "                                   \n",
    "        G.nodes[node][\"geneIDs\"] = node_GIDs_split\n",
    "        #print(\"new IDs: \", G.nodes[node].get(\"geneIDs\", \"error\"))\n",
    "\n",
    "    return G\n",
    "\n",
    "sim_graph_04 = indGID_to_allGID(sim_graph_04, matches_g1)\n",
    "sim_graph_59 = indGID_to_allGID(sim_graph_59, matches_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c30cfa-b72d-4c0c-878e-5119ba478624",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### DEPRECATED\n",
    "\n",
    "\n",
    "# change geneIDs to unique geneIDs (since geneIDs not unique between graphs)\n",
    "\n",
    "def geneID_to_uGID(G, graph_id):\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        node_GIDs = G.nodes[node].get(\"geneIDs\", \"error\")\n",
    "        #print(f\"node_GIDs: {node_GIDs}\")\n",
    "        node_GIDs_split = \";\".join(str(f\"uGID_{graph_id}_{GID}\") for GID in node_GIDs.split(\";\"))\n",
    "        G.nodes[node][\"geneIDs\"] = node_GIDs_split\n",
    "        #print(\"UGIDs: \", G.nodes[node].get(\"geneIDs\", \"error\"))\n",
    "\n",
    "    return G\n",
    "\n",
    "sim_graph_04 = geneID_to_uGID(sim_graph_04, \"G1\")\n",
    "sim_graph_59 = geneID_to_uGID(sim_graph_59, \"G2\")\n",
    "\n",
    "#print(sim_graph_04.nodes[0].get(\"geneIDs\", \"error\"))\n",
    "#print(sim_graph_59.nodes[3824].get(\"geneIDs\", \"error\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765cf45-0350-4b2c-88db-f4f078f2cff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### map nodes from ggcaller graphs to the COG labels in the centroid from pangenome\n",
    "\n",
    "# read into df\n",
    "# each \"group_\" refers to the centroid of that group in the pan_genomes_reference.fa\n",
    "mmseqs = pd.read_csv('/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_merged/mmseqs_alignments.m8', sep='\\t')\n",
    "\n",
    "### match hits from mmseqs\n",
    "\n",
    "# change the second graph node names to the first graph node names for nodes that match according to mmseqs\n",
    "\n",
    "# make sure metrics are numeric\n",
    "mmseqs[\"fident\"] = pd.to_numeric(mmseqs[\"fident\"], errors='coerce')\n",
    "mmseqs[\"evalue\"] = pd.to_numeric(mmseqs[\"evalue\"], errors='coerce')\n",
    "mmseqs[\"tlen\"] = pd.to_numeric(mmseqs[\"tlen\"], errors='coerce')\n",
    "mmseqs[\"qlen\"] = pd.to_numeric(mmseqs[\"qlen\"], errors='coerce')\n",
    "mmseqs[\"nident\"] = pd.to_numeric(mmseqs[\"nident\"], errors='coerce')\n",
    "\n",
    "# filter for nt identity >= 98% (global) and length difference <= 5%\n",
    "max_len = np.maximum(mmseqs['tlen'], mmseqs['qlen'])\n",
    "nt_identity = mmseqs['nident'] / max_len  >= 0.98\n",
    "nt_identity = max_len / max_len  >= 0.98\n",
    "len_dif = 1-(np.abs(mmseqs['tlen'] - mmseqs['qlen']) / max_len) >= 0.95\n",
    "\n",
    "scores = nt_identity & len_dif\n",
    "mmseqs = mmseqs[scores].copy()\n",
    "\n",
    "# iterate over target with each unique value of target, and pick the match with the highest fident; if multiple, pick the one with the smaller E value\n",
    "\n",
    "# sort by fident (highest first) and evalue (lowest first)\n",
    "mmseqs_sorted = mmseqs.sort_values(by=[\"fident\", \"evalue\"], ascending=[False, True])\n",
    "\n",
    "# only keep the first occurrence per unique target (highest fident then smallest evalue if tie)\n",
    "mmseqs_filtered = mmseqs_sorted.groupby(\"target\", as_index=False).first()\n",
    "\n",
    "# remove \"group_\" prefix, keep as string\n",
    "#mmseqs_filtered = mmseqs_filtered.applymap(lambda x: x.replace(\"group_\", \"\") if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f503169-1ba1-4e3e-b363-3ceb02a9830d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in mmseqs, the first graph entered (in this case graph_1) is the query and the second entered (in this case graph_2) is the target\n",
    "# so graph_1 is our query in mmseqs and the basegraph in the tokenized merge\n",
    "\n",
    "# when iterating over the graph used to append the basegraph (graph_2), we want to match nodes according to their graph_1 identity\n",
    "# so we first need to replace all graph_2 nodes with graph_1 node ids\n",
    "\n",
    "### THE NAME (\"group_1\") AND THE LABEL ('484') ARE DIFFERENT AND A NUMERIC STRING WILL CALL THE LABEL (not index)\n",
    "\n",
    "mapping_groups_1 = dict()\n",
    "for node in sim_graph_04.nodes():\n",
    "    node_group = sim_graph_04.nodes[node].get(\"name\", \"error\")\n",
    "    #print(f\"graph: 1, node_index_id: {node}, node_group_id: {node_group}\")\n",
    "    mapping_groups_1[int(node)] = str(node_group)\n",
    "\n",
    "groupmapped_graph_1 = nx.relabel_nodes(sim_graph_04, mapping_groups_1, copy=True)\n",
    "\n",
    "mapping_groups_2 = dict()\n",
    "for node in sim_graph_59.nodes():\n",
    "    node_group = sim_graph_59.nodes[node].get(\"name\", \"error\")\n",
    "    #print(f\"graph: 1, node_index_id: {node}, node_group_id: {node_group}\")\n",
    "    mapping_groups_2[int(node)] = str(node_group)\n",
    "\n",
    "groupmapped_graph_2 = nx.relabel_nodes(sim_graph_59, mapping_groups_2, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da049b9d-4806-4d13-9f67-9b294051c348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### map filtered mmseqs2 hits to mapping of nodes between graphs\n",
    "\n",
    "# mapping format: dictionary with old labels as keys and new labels as values\n",
    "\n",
    "# convert df to dictionary with \"target\" as keys and \"query\" as values\n",
    "mapping = dict(zip(mmseqs_filtered[\"target\"], mmseqs_filtered[\"query\"]))\n",
    "\n",
    "### to avoid matching nodes from target that have the same group_id but are not the same:\n",
    "# append all nodes in query graph with _query\n",
    "# append all query nodes in target graph with _query (for later matching)\n",
    "\n",
    "mapping = {key: f\"{value}_query\" for key, value in mapping.items()}\n",
    "\n",
    "# relabel target graph\n",
    "relabeled_graph_2 = nx.relabel_nodes(groupmapped_graph_2, mapping, copy=True)\n",
    "\n",
    "# relabel query graph\n",
    "mapping_query = dict(zip(groupmapped_graph_1.nodes, groupmapped_graph_1.nodes))\n",
    "\n",
    "mapping_query = {key: f\"{value}_query\" for key, value in mapping_query.items()}\n",
    "\n",
    "relabeled_graph_1 = nx.relabel_nodes(groupmapped_graph_1, mapping_query, copy=True)\n",
    "\n",
    "### need to do this to edges as well\n",
    "#print(relabeled_graph_1.edges)\n",
    "#print(relabeled_graph_2.edges)\n",
    "# looks like this is done automatically\n",
    "\n",
    "# now we can modify the tokenized code to iterate like usual, adding new node if string doesn't contain \"_query\"\n",
    "# and merging the nodes that both end in \"_query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8204f-aeb5-4d71-82f0-e6be259a5192",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to double-check above\n",
    "if set(relabeled_graph_1.nodes) == set(relabeled_graph_2.nodes):\n",
    "    print(\"The node sets are identical.\")\n",
    "else:\n",
    "    print(\"The node sets are different.\")\n",
    "    \n",
    "nodes_1 = set(relabeled_graph_1.nodes)\n",
    "nodes_2 = set(relabeled_graph_2.nodes)\n",
    "\n",
    "common_nodes = nodes_1 & nodes_2  # Intersection (nodes in both graphs)\n",
    "only_in_graph_1 = nodes_1 - nodes_2  # Nodes only in Graph 1\n",
    "only_in_graph_2 = nodes_2 - nodes_1  # Nodes only in Graph 2\n",
    "\n",
    "print(f\"Common Nodes: {common_nodes}\")\n",
    "print(f\"Nodes only in Graph 1: {only_in_graph_1}\")\n",
    "print(f\"Nodes only in Graph 2: {only_in_graph_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04735f5-f20d-4ed4-8157-8a73dada1ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add metadata when merging\n",
    "\n",
    "### FROM PANAROO MERGE_NODES.PY\n",
    "\n",
    "import itertools\n",
    "from collections import Counter\n",
    "#from .isvalid import del_dups\n",
    "import numpy as np\n",
    "#from intbitset import intbitset\n",
    "\n",
    "\n",
    "def gen_node_iterables(G, nodes, feature, split=None):\n",
    "    for n in nodes:\n",
    "        if split is None:\n",
    "            yield G.nodes[n][feature]\n",
    "        else:\n",
    "            yield G.nodes[n][feature].split(split)\n",
    "\n",
    "\n",
    "def gen_edge_iterables(G, edges, feature):\n",
    "    for e in edges:\n",
    "        yield G[e[0]][e[1]][feature]\n",
    "\n",
    "\n",
    "def temp_iter(list_list):\n",
    "    for n in list_list:\n",
    "        yield n\n",
    "\n",
    "\n",
    "def iter_del_dups(iterable):\n",
    "    seen = {}\n",
    "    for f in itertools.chain.from_iterable(iterable):\n",
    "        seen[f] = None\n",
    "    return (list(seen.keys()))\n",
    "\n",
    "\n",
    "def del_dups(iterable):\n",
    "    seen = {}\n",
    "    for f in iterable:\n",
    "        seen[f] = None\n",
    "    return (list(seen.keys()))\n",
    "\n",
    "\n",
    "def merge_node_cluster(G,\n",
    "                       nodes,\n",
    "                       newNode,\n",
    "                       multi_centroid=True,\n",
    "                       check_merge_mems=True):\n",
    "\n",
    "    if check_merge_mems:\n",
    "        mem_count = Counter(\n",
    "            itertools.chain.from_iterable(\n",
    "                gen_node_iterables(G, nodes, 'members')))\n",
    "        # genome IDs can be identical since they're being merged, so comment out this bit\n",
    "        #if max(mem_count.values()) > 1:\n",
    "            #raise ValueError(\"merging nodes with the same genome IDs!\")\n",
    "\n",
    "    # take node with most support as the 'consensus'\n",
    "    nodes = sorted(nodes, key=lambda x: G.nodes[x]['size'])\n",
    "\n",
    "    # First create a new node and combine the attributes\n",
    "    dna = iter_del_dups(gen_node_iterables(G, nodes, 'dna'))\n",
    "    maxLenId = 0\n",
    "    max_l = 0\n",
    "    for i, s in enumerate(dna):\n",
    "        if len(s) >= max_l:\n",
    "            max_l = len(s)\n",
    "            maxLenId = i\n",
    "            \n",
    "    members = G.nodes[nodes[0]]['members'].copy()\n",
    "    for n in nodes[1:]:\n",
    "        members = list(dict.fromkeys(G.nodes[n]['members']))\n",
    "\n",
    "    if multi_centroid:\n",
    "        mergedDNA = any(gen_node_iterables(G, nodes, 'mergedDNA'))\n",
    "    else:\n",
    "        mergedDNA = True\n",
    "\n",
    "    G.add_node(\n",
    "        newNode,\n",
    "        size=len(members),\n",
    "        centroid=iter_del_dups(gen_node_iterables(G, nodes, 'centroid')),\n",
    "        maxLenId=maxLenId,\n",
    "        members=members,\n",
    "        seqIDs=set(iter_del_dups(gen_node_iterables(G, nodes, 'seqIDs'))),\n",
    "        hasEnd=any(gen_node_iterables(G, nodes, 'hasEnd')),\n",
    "        protein=iter_del_dups(gen_node_iterables(G, nodes, 'protein')),\n",
    "        dna=dna,\n",
    "        geneIDs=\";\".join(iter_del_dups(gen_node_iterables(G, nodes, 'geneIDs', split=\";\"))),  # <-- added geneID merge for uGIDs\n",
    "        annotation=\";\".join(\n",
    "            iter_del_dups(gen_node_iterables(G, nodes, 'annotation',\n",
    "                                             split=\";\"))),\n",
    "        description=\";\".join(\n",
    "            iter_del_dups(\n",
    "                gen_node_iterables(G, nodes, 'description', split=\";\"))),\n",
    "        lengths=list(\n",
    "            itertools.chain.from_iterable(\n",
    "                gen_node_iterables(G, nodes, 'lengths'))),\n",
    "        longCentroidID=max(gen_node_iterables(G, nodes, 'longCentroidID')),\n",
    "        paralog=any(gen_node_iterables(G, nodes, 'paralog')),\n",
    "        mergedDNA=mergedDNA)\n",
    "    if \"prevCentroids\" in G.nodes[nodes[0]]:\n",
    "        G.nodes[newNode]['prevCentroids'] = \";\".join(\n",
    "            set(\n",
    "                iter_del_dups(\n",
    "                    gen_node_iterables(G, nodes, 'prevCentroids', split=\";\"))))\n",
    "\n",
    "    # Now iterate through neighbours of each node and add them to the new node\n",
    "    merge_nodes = set(nodes)\n",
    "    for node in nodes:\n",
    "        for neighbour in G.neighbors(node):\n",
    "            if neighbour in merge_nodes: continue\n",
    "            if G.has_edge(newNode, neighbour):\n",
    "                G[newNode][neighbour]['members'] = list(dict.fromkeys(G[node][neighbour][\n",
    "                    'members']))\n",
    "                G[newNode][neighbour]['size'] = len(G[newNode][neighbour]['members'])\n",
    "            else:\n",
    "                G.add_edge(newNode,\n",
    "                           neighbour,\n",
    "                           size=G[node][neighbour]['size'],\n",
    "                           members=G[node][neighbour]['members'])\n",
    "\n",
    "    # remove old nodes from Graph\n",
    "    G.remove_nodes_from(nodes)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def delete_node(G, node):\n",
    "    # add in new edges\n",
    "    for mem in G.nodes[node]['members']:\n",
    "        mem_edges = list(\n",
    "            set([e[1] for e in G.edges(node) if mem in G.edges[e]['members']]))\n",
    "        if len(mem_edges) < 2: continue\n",
    "        for n1, n2 in itertools.combinations(mem_edges, 2):\n",
    "            if G.has_edge(n1, n2):\n",
    "                G[n1][n2]['members'] |= intbitset([mem])\n",
    "                G[n1][n2]['size'] = len(G[n1][n2]['members'])\n",
    "            else:\n",
    "                G.add_edge(n1, n2, size=1, members=intbitset([mem]))\n",
    "\n",
    "    # now remove node\n",
    "    G.remove_node(node)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def remove_member_from_node(G, node, member):\n",
    "\n",
    "    # add in replacement edges if required\n",
    "    mem_edges = list(\n",
    "        set([e[1] for e in G.edges(node) if member in G.edges[e]['members']]))\n",
    "    if len(mem_edges) > 1:\n",
    "        for n1, n2 in itertools.combinations(mem_edges, 2):\n",
    "            if G.has_edge(n1, n2):\n",
    "                G[n1][n2]['members'] |= intbitset([member])\n",
    "                G[n1][n2]['size'] = len(G[n1][n2]['members'])\n",
    "            else:\n",
    "                G.add_edge(n1, n2, size=1, members=intbitset([member]))\n",
    "\n",
    "    # remove member from node\n",
    "    G.nodes[node]['members'].discard(member)\n",
    "    G.nodes[node]['seqIDs'] = set([\n",
    "        sid for sid in G.nodes[node]['seqIDs']\n",
    "        if sid.split(\"_\")[0] != str(member)\n",
    "    ])\n",
    "    G.nodes[node]['size'] -= 1\n",
    "\n",
    "    # remove member from edges of node\n",
    "    edges_to_remove = []\n",
    "    for e in G.edges(node):\n",
    "        if member in G.edges[e]['members']:\n",
    "            if len(G.edges[e]['members']) == 1:\n",
    "                edges_to_remove.append(e)\n",
    "            else:\n",
    "                G.edges[e]['members'].discard(member)\n",
    "                G.edges[e]['size'] = len(G.edges[e]['members'])\n",
    "    for e in edges_to_remove:\n",
    "        G.remove_edge(*e)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2f936-c5fa-4a4f-bcd5-d0fdced6f754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_graph = relabeled_graph_1\n",
    "print(\"num nodes (before):\", len(merged_graph.nodes))\n",
    "print(\"num edges (before):\", len(merged_graph.edges))\n",
    "\n",
    "merged_graph = nx.compose_all([relabeled_graph_1, relabeled_graph_2])\n",
    "\n",
    "# merge the two sets of unique nodes into one set of unique nodes\n",
    "for node in relabeled_graph_2.nodes:\n",
    "\n",
    "    if merged_graph.has_node(node):\n",
    "        #print(f\"Part of query graph: {node}\")\n",
    "\n",
    "        # merge metadata from two nodes together\n",
    "        merge_node_cluster(merged_graph, [node, node], f\"{node}_merged\")\n",
    "\n",
    "print(\"num nodes (after):\", len(merged_graph.nodes))\n",
    "\n",
    "print(\"relabeled_graph_2 edges:\", len(relabeled_graph_2.edges))\n",
    "\n",
    "print(\"num edges (after):\", len(merged_graph.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e9f7b-174f-41e9-b2f8-efdff29d7cba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###### ORIGINAL\n",
    "\n",
    "# for adding to genome ids of second graph\n",
    "n_genomes_basegraph = 45\n",
    "\n",
    "# create merged graph\n",
    "\n",
    "def main():\n",
    "\n",
    "    merged_graph = relabeled_graph_1\n",
    "    print(\"num nodes (before):\", len(merged_graph.nodes))\n",
    "\n",
    "    # merge the two sets of unique nodes into one set of unique nodes\n",
    "    for node in relabeled_graph_2.nodes:\n",
    "        if node != \"_\":\n",
    "            print(node)\n",
    "            # remove negative but keep as string (otherwise has_node thinks\n",
    "            # that node name is the node index)\n",
    "            #node = int(node)\n",
    "            #node = abs(node)\n",
    "            #node = str(node)\n",
    "            if merged_graph.has_node(node):\n",
    "                \n",
    "                # add metadata\n",
    "                \n",
    "                print(\"Part of query graph. Adding metadata.\")\n",
    "                \n",
    "                #merged_graph.nodes[node]['total_sim'] += \";\"\n",
    "                #to_append = \";\".join(str(int(num) + n_genomes_basegraph) for num in sim_graph_59.nodes[node]['total_sim'].split(\";\"))\n",
    "                #merged_graph.nodes[node]['total_sim'] += to_append\n",
    "\n",
    "                #print(\"merged total (after):\", merged_graph.nodes[node]['total_sim'])\n",
    "\n",
    "                #set_old = set(merged_graph.nodes[node]['members_sim']) - set(\";\")    \n",
    "                #set_to_add = set(sim_graph_59.nodes[node]['members_sim']) - set(\";\")\n",
    "                #set_add_reindexed = {str(int(num) + n_genomes_basegraph) for num in set_to_add}\n",
    "                #set_total = set_old | set_add_reindexed\n",
    "                #set_total_str = \";\".join(set_total)\n",
    "                #merged_graph.nodes[node]['members_sim'] = set_total_str\n",
    "                \n",
    "                #print(\"merged members (after):\", merged_graph.nodes[node]['members_sim'])\n",
    "                \n",
    "            if not merged_graph.has_node(node):\n",
    "                \n",
    "                print(\"Add new node to graph.\")\n",
    "                \n",
    "                merged_graph.add_node(node)\n",
    "                #members_sim=[],\n",
    "                #total_sim=[])\n",
    "\n",
    "                #### REINDEX\n",
    "                \n",
    "                #members_sim_unindexed = sim_graph_59.nodes[node]['members_sim']\n",
    "                #print(\"members_sim_unindexed:\", members_sim_unindexed)\n",
    "                #members_sim_indexed = \";\".join(str(int(num) + n_genomes_basegraph) for num in members_sim_unindexed.split(\";\"))\n",
    "                #merged_graph.nodes[node]['members_sim'] = members_sim_indexed\n",
    "                #print(\"members sim indexed\", merged_graph.nodes[node]['members_sim'])\n",
    "\n",
    "                #total_sim_unindexed = sim_graph_59.nodes[node]['total_sim']\n",
    "                #print(\"total_sim_unindexed:\", total_sim_unindexed)\n",
    "                #total_sim_indexed = \";\".join(str(int(num) + n_genomes_basegraph) for num in total_sim_unindexed.split(\";\"))\n",
    "                #merged_graph.nodes[node]['total_sim'] = total_sim_indexed\n",
    "                #print(\"total_sim indexed:\", merged_graph.nodes[node]['total_sim'])\n",
    "\n",
    "    print(\"num nodes (after):\", len(merged_graph.nodes))\n",
    "\n",
    "    print(\"relabeled_graph_2 edges:\", len(relabeled_graph_2.edges))\n",
    "    print(\"num edges (before):\", len(merged_graph.edges))\n",
    "\n",
    "    # merge the two sets of edges into one set of edges,\n",
    "    # removing duplicates (including those with different strandedness)\n",
    "\n",
    "    for edge in relabeled_graph_2.edges:\n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        \n",
    "        # strip \"_query\" if it exists at the end of the string before interpreting order, then re-add\n",
    "        if \"_query\" in u:\n",
    "            u = u.removesuffix(\"_query\")\n",
    "            u = int(u)\n",
    "                if \"_query\" in v:\n",
    "                    v = v.removesuffix(\"_query\")\n",
    "                    v = int(v)\n",
    "            \n",
    "                    # add in absolute order\n",
    "                    first = min(abs(u), abs(v))\n",
    "                    second = max(abs(u), abs(v))\n",
    "\n",
    "                    # make strings (same reason as with nodes, bc otherwise they're interpreted as indexes)\n",
    "                    first = f\"{first}_query\"\n",
    "                    second = f\"{second}_query\"\n",
    "                    \n",
    "                if \"_query\" not in v:\n",
    "                    v = int(v)\n",
    "            \n",
    "                    # add in absolute order\n",
    "                    first = min(abs(u), abs(v))\n",
    "                    second = max(abs(u), abs(v))\n",
    "\n",
    "                    # make strings (same reason as with nodes, bc otherwise they're interpreted as indexes)\n",
    "                    if first == abs(u):\n",
    "                        first = f\"{first}_query\"\n",
    "                        \n",
    "                    if second == abs(u):\n",
    "                        second = f\"{second}_query\"\n",
    "            \n",
    "            if \"_query\" not in u:\n",
    "            u = int(u)\n",
    "            \n",
    "                if \"_query\" in v:\n",
    "                    v = int(v)\n",
    "            \n",
    "                    # add in absolute order\n",
    "                    first = min(abs(u), abs(v))\n",
    "                    second = max(abs(u), abs(v))\n",
    "\n",
    "                    # make strings (same reason as with nodes, bc otherwise they're interpreted as indexes)\n",
    "                    if first == abs(v):\n",
    "                        first = f\"{first}_query\"\n",
    "                        \n",
    "                    if second == abs(v):\n",
    "                        second = f\"{second}_query\"\n",
    "            \n",
    "                if \"_query\" not in v: \n",
    "                    v = int(v)\n",
    "            \n",
    "                    # add in absolute order\n",
    "                    first = min(abs(u), abs(v))\n",
    "                    second = max(abs(u), abs(v))\n",
    "\n",
    "                    # make strings (same reason as with nodes, bc otherwise they're interpreted as indexes)\n",
    "                    first = str(first)\n",
    "                    second = str(second)\n",
    "\n",
    "            if merged_graph.has_edge(first, second):\n",
    "                \n",
    "                print(\"Edge exists.\")\n",
    "                \n",
    "                #print(\"merged edge existing:\", merged_graph.edges[edge[0], edge[1]]['members_sim'])\n",
    "\n",
    "                #set_old = set(merged_graph.edges[edge[0], edge[1]]['members_sim']) - set(\";\")    \n",
    "                #set_to_add = set(sim_graph_59.edges[edge[0], edge[1]]['members_sim']) - set(\";\")\n",
    "                #set_add_reindexed = {str(int(num) + n_genomes_basegraph) for num in set_to_add}\n",
    "                #set_total = set_old | set_add_reindexed\n",
    "                #set_total_str = \";\".join(set_total)\n",
    "                #merged_graph.edges[edge[0], edge[1]]['members_sim'] = set_total_str\n",
    "                \n",
    "                #print(\"merged edge updated:\", merged_graph.edges[edge[0], edge[1]]['members_sim'])\n",
    "\n",
    "                #print(\"merged edge total existing:\", merged_graph.edges[edge[0], edge[1]]['total_sim'])\n",
    "\n",
    "                #merged_graph.edges[edge[0], edge[1]]['total_sim'] += \";\"\n",
    "                #to_append = \";\".join(str(int(num) + n_genomes_basegraph) for num in sim_graph_59.edges[edge[0], edge[1]]['total_sim'].split(\";\"))\n",
    "                #merged_graph.edges[edge[0], edge[1]]['total_sim'] += to_append\n",
    "                \n",
    "                #print(\"merged edge total updated:\", merged_graph.edges[edge[0], edge[1]]['total_sim'])\n",
    "\n",
    "                #print(\"strand sim before:\", merged_graph.edges[edge[0], edge[1]]['strand_sim'])\n",
    "\n",
    "                #merged_graph.edges[edge[0], edge[1]]['strand_sim'] += \";\"\n",
    "                #merged_graph.edges[edge[0], edge[1]]['strand_sim'] += sim_graph_59.edges[edge[0], edge[1]]['strand_sim']\n",
    "                \n",
    "                #print(\"strand sim after:\", merged_graph.edges[edge[0], edge[1]]['strand_sim'])\n",
    "\n",
    "            if not merged_graph.has_edge(first, second):\n",
    "                merged_graph.add_edge(first, second)\n",
    "                                #members_sim = [],\n",
    "                                #strand_sim=[],\n",
    "                                #total_sim=[])\n",
    "                            \n",
    "                print(\"Added edge.\")\n",
    "\n",
    "                ### REINDEX (strand doesn't need to be indexed)\n",
    "\n",
    "                #members_sim_unindexed = sim_graph_59.edges[edge[0], edge[1]]['members_sim']\n",
    "                #print(\"members_sim_unindexed:\", members_sim_unindexed)\n",
    "                #members_sim_indexed = \";\".join(str(int(num) + n_genomes_basegraph) for num in members_sim_unindexed.split(\";\"))\n",
    "                #merged_graph.edges[edge[0], edge[1]]['members_sim'] = members_sim_indexed\n",
    "                #print(\"members sim indexed\", merged_graph.edges[edge[0], edge[1]]['members_sim'])\n",
    "\n",
    "                #total_sim_unindexed = sim_graph_59.edges[edge[0], edge[1]]['total_sim']\n",
    "                #print(\"total_sim_unindexed:\", members_sim_unindexed)\n",
    "                #total_sim_indexed = \";\".join(str(int(num) + n_genomes_basegraph) for num in total_sim_unindexed.split(\";\"))\n",
    "                #merged_graph.edges[edge[0], edge[1]]['total_sim'] = total_sim_indexed\n",
    "                #print(\"total_sim indexed:\", merged_graph.edges[edge[0], edge[1]]['total_sim'])\n",
    "\n",
    "                #print(\"strand sim before:\", sim_graph_59.edges[edge[0], edge[1]]['strand_sim'])\n",
    "                #merged_graph.edges[edge[0], edge[1]]['strand_sim'] = sim_graph_59.edges[edge[0], edge[1]]['strand_sim']\n",
    "                #print(\"strand sim after:\", merged_graph.edges[edge[0], edge[1]]['strand_sim'])\n",
    "\n",
    "    print(\"num edges (after):\", len(merged_graph.edges))\n",
    "    \n",
    "    nx.write_gml(merged_graph, \"../../graph_merged/merged_graph.gml\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ca615-08a8-463c-9e00-df7b55c62ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot merged graph\n",
    "\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "nx.draw(merged_graph, with_labels=False, node_color='red', edge_color='gray', node_size = 2)\n",
    "plt.show()\n",
    "\n",
    "# plot joint graph\n",
    "\n",
    "graph_all = nx.read_gml(\"/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_all/final_graph.gml\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "nx.draw(graph_all, with_labels=False, node_color='red', edge_color='gray', node_size = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6f1bd-ff6d-4243-a237-898532a071c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add metadata (geneIDs)\n",
    "\n",
    "gene_ids = {}\n",
    "    centroids_to_nodes = defaultdict(list)\n",
    "    for cluster in clusters:\n",
    "        for sid in cluster:\n",
    "            gene_ids[sid] = cluster[0]\n",
    "    all_centroids = set(list(gene_ids.values()))\n",
    "    \n",
    "for G in graphs:\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node][\"geneID\"] = list(\n",
    "            set([\n",
    "                gene_ids[sid] for sid in G.nodes[node]['seqIDs']\n",
    "                if \"refound\" not in sid\n",
    "            ]))\n",
    "        for sid in G.nodes[node][\"centroid\"]:\n",
    "            centroids_to_nodes[sid].append(node)\n",
    "            \n",
    "#        G.nodes[node][\"dna\"] = [\n",
    "#            centroid_to_seqs[sid][1] for sid in G.nodes[node][\"centroid\"]\n",
    "#        ]#\n",
    "#\n",
    "#        G.nodes[node][\"protein\"] = [\n",
    "#            centroid_to_seqs[sid][0] for sid in G.nodes[node][\"centroid\"]\n",
    "#        ]\n",
    "#        G.nodes[node][\"longCentroidID\"] = max([\n",
    "#            (len(seq), sid) for seq, sid in zip(G.nodes[node][\"dna\"],\n",
    "#                                                G.nodes[node][\"centroid\"])\n",
    " #       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1b6b4-7839-46ba-989a-cac6d25c9750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in graph_all\n",
    "\n",
    "graph_all = [\"/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/COG_merge/graph_all/final_graph.gml\"]\n",
    "\n",
    "graph_all, isolate_names, id_mapping = load_graphs(graph_all)\n",
    "graph_all = graph_all[0]\n",
    "\n",
    "#list(graph_all.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0f3a4-bb98-40ab-8f1d-57266e131877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define functions to generate dna hashes from node metadata\n",
    "\n",
    "def match_uGID_sequences(G, sequences_g1, sequences_g2):\n",
    "    \n",
    "    # allows for mapping sequence GIDs to node UGIDs\n",
    "    # only needed for G1 and G2 components of merged graph, not for graph_all\n",
    "    \n",
    "    # append uGID sequence keys (non-unique geneIDs) with uGID prefix\n",
    "    prefix = \"uGID_G1_\"\n",
    "    sequences_g1 = {f\"{prefix}{key}\": value for key, value in sequences_g1.items()}\n",
    "    prefix = \"uGID_G2_\"\n",
    "    sequences_g2 = {f\"{prefix}{key}\": value for key, value in sequences_g2.items()}\n",
    "\n",
    "    # check that uGIDs are unique between sequence datasets\n",
    "    overlapping_keys = set(sequences_g1.keys()) & set(sequences_g2.keys())\n",
    "    if overlapping_keys:\n",
    "        raise ValueError(f\"Duplicate keys found: {overlapping_keys}\")\n",
    "            \n",
    "    # combine uGID-labeled sequences from G1 and G2\n",
    "    return ({**sequences_g1, **sequences_g2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859d33b-4a69-4dfc-b67f-53db84a67b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### map geneIDs (joint graph) and uGIDs (merged graph) to DNA sequences\n",
    "\n",
    "# read in gene sequences\n",
    "sequences_all = pd.read_csv('/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_all/gene_data.csv', usecols=[\"clustering_id\", \"dna_sequence\"])\n",
    "#sequences_g1 = pd.read_csv('/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_1/gene_data.csv', usecols=[\"clustering_id\", \"dna_sequence\"])\n",
    "#sequences_g2 = pd.read_csv('/nfs/research/jlees/jacqueline/atb_analyses/merge_tests/staph_merge/graph_2/gene_data.csv', usecols=[\"clustering_id\", \"dna_sequence\"])\n",
    "\n",
    "#print(sequences_g1)\n",
    "\n",
    "# make into dictionaries\n",
    "sequences_all = dict(zip(sequences_all[\"clustering_id\"], sequences_all[\"dna_sequence\"]))\n",
    "#sequences_g1 = dict(zip(sequences_g1[\"clustering_id\"], sequences_g1[\"dna_sequence\"]))\n",
    "#sequences_g2 = dict(zip(sequences_g2[\"clustering_id\"], sequences_g2[\"dna_sequence\"]))\n",
    "\n",
    "#print(sequences_g1[\"0_0_0\"])\n",
    "\n",
    "#sequences_merged = match_uGID_sequences(merged_graph, sequences_g1, sequences_g2)\n",
    "\n",
    "#print(sequences_merged[\"uGID_G1_0_0_0\"])\n",
    "\n",
    "#######\n",
    "\n",
    "#same = (Counter(sequences_all.values()) == Counter(sequences_merged.values()))\n",
    "#print(same)\n",
    "\n",
    "#print(len(sequences_all.values()), len(sequences_merged.values()))\n",
    "\n",
    "# Convert values to tuples (in case they're lists or other unhashable types)\n",
    "#values_all = set(tuple(v) for v in sequences_all.values())\n",
    "#values_merged = set(tuple(v) for v in sequences_merged.values())\n",
    "\n",
    "#only_in_all = values_all - values_merged\n",
    "#only_in_merged = values_merged - values_all\n",
    "\n",
    "#print(\"Values only in sequences_all:\", only_in_all)\n",
    "#print(\"Values only in sequences_merged:\", only_in_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebcde4-ca13-4cd1-bfdd-f951cfc84cea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### hash DNA sequences\n",
    "        \n",
    "### use aaHash here instead!!\n",
    "        \n",
    "import hashlib\n",
    "\n",
    "sequences_all = {key: hashlib.md5(str(value).encode()).hexdigest() for key, value in sequences_all.items()}\n",
    "#sequences_merged = {key: hashlib.md5(str(value).encode()).hexdigest() for key, value in sequences_merged.items()}\n",
    "\n",
    "#print(sequences_all)\n",
    "print(len(sequences_all.values()))\n",
    "\n",
    "#print(list(sequences_merged.values()))\n",
    "\n",
    "print(len(graph_all.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f1759-7e06-4a60-8d6f-070c6161fd58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### get dictionary with a hashed DNA sequence as the key and its cluster as the value\n",
    "\n",
    "# get list of geneIDs/uGIDs in each node\n",
    "\n",
    "def get_geneIDs_in_nodes(G):  \n",
    "\n",
    "    # get individual gene IDs as keys, and the cluster each geneID belongs to as its value\n",
    "    dictionary = {}\n",
    "    total_geneids = 0\n",
    "    total_nodes = 0\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        gene_ids = G.nodes[node].get(\"geneIDs\", \"error\")\n",
    "        #print(f\"graph: 1, node_index_id: {node}, node_group_id: {node_group}\")\n",
    "        gene_ids = list(gene_ids.split(\";\"))\n",
    "        \n",
    "        #print(f\"gene ids: {gene_ids}\")\n",
    "        #print(f\"len gene ids: {len(gene_ids)}\")\n",
    "        #print(f\"node: {node}\")\n",
    "        \n",
    "        total_geneids += len(gene_ids)\n",
    "        total_nodes += 1\n",
    "        \n",
    "        \n",
    "        for GID in gene_ids:\n",
    "            dictionary[GID] = node\n",
    "     \n",
    "    print(f\"total geneids in all nodes: {total_geneids}\")\n",
    "    print(f\"total nodes in graph: {total_nodes}\")\n",
    "        \n",
    "    return dictionary\n",
    "\n",
    "#cluster_dict_merged = get_geneIDs_in_nodes(merged_graph)\n",
    "#cluster_dict_all = get_geneIDs_in_nodes(graph_all)\n",
    "#print(cluster_dict_all)\n",
    "\n",
    "#print(cluster_dict_all['0_0_0'])\n",
    "#print(\"Key with the most values:\", max_key)\n",
    "\n",
    "#print(cluster_dict_merged.keys())\n",
    "#print(len(cluster_dict_merged.values()), len(cluster_dict_all.values()))\n",
    "\n",
    "#print(cluster_dict_all)\n",
    "\n",
    "\n",
    "\n",
    "def get_seqIDs_in_nodes(G):  \n",
    "\n",
    "    # get individual gene IDs as keys, and the cluster each geneID belongs to as its value\n",
    "    dictionary = {}\n",
    "    total_seqids = 0\n",
    "    total_nodes = 0\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        seq_ids = G.nodes[node].get(\"seqIDs\", \"error\")\n",
    "        #print(f\"graph: 1, node_index_id: {node}, node_group_id: {node_group}\")\n",
    "        seq_ids = list(seq_ids)\n",
    "        \n",
    "        #print(f\"gene ids: {gene_ids}\")\n",
    "        #print(f\"len gene ids: {len(gene_ids)}\")\n",
    "        #print(f\"node: {node}\")\n",
    "        \n",
    "        total_seqids += len(seq_ids)\n",
    "        total_nodes += 1\n",
    "        \n",
    "        \n",
    "        for SID in seq_ids:\n",
    "            dictionary[SID] = node\n",
    "     \n",
    "    print(f\"total geneids in all nodes: {total_seqids}\")\n",
    "    print(f\"total nodes in graph: {total_nodes}\")\n",
    "        \n",
    "    return dictionary\n",
    "\n",
    "cluster_dict_seqs_merged = get_seqIDs_in_nodes(merged_graph)\n",
    "cluster_dict_seqs_all = get_seqIDs_in_nodes(graph_all)\n",
    "#print(cluster_dict_seqs_all)\n",
    "\n",
    "#print(cluster_dict_all['0_0_0'])\n",
    "#print(\"Key with the most values:\", max_key)\n",
    "\n",
    "#print(cluster_dict_merged.keys())\n",
    "#print(len(cluster_dict_merged.values()), len(cluster_dict_all.values()))\n",
    "\n",
    "# swap geneIDs/uGIDs for hashed DNA (remember we are mapping the two runs by hashed DNA, not by GID)\n",
    "\n",
    "def match_hash_and_cluster(sequence, cluster):\n",
    "    \n",
    "    # Use keys from dict1 as the base order\n",
    "    common_keys = [key for key in sequence if key in cluster]\n",
    "\n",
    "    row1 = [sequence[key] for key in common_keys]\n",
    "    row2 = [cluster[key] for key in common_keys]\n",
    "\n",
    "    array_2d = pd.DataFrame([row1, row2])\n",
    "    \n",
    "    return array_2d\n",
    "    \n",
    "#rand_input_merged = match_hash_and_cluster(sequences_all, cluster_dict_merged)\n",
    "#rand_input_all = match_hash_and_cluster(sequences_all, cluster_dict_all)\n",
    "\n",
    "def dict_to_2d_array(d):\n",
    "    row_keys = []\n",
    "    row_values = []\n",
    "\n",
    "    for key, value in d.items():\n",
    "        # If the value is a list or tuple, assume multiple values\n",
    "        if isinstance(value, (list, tuple)):\n",
    "            for v in value:\n",
    "                row_keys.append(key)\n",
    "                row_values.append(v)\n",
    "        else:\n",
    "            row_keys.append(key)\n",
    "            row_values.append(value)\n",
    "\n",
    "    return pd.DataFrame([row_keys, row_values])\n",
    "\n",
    "rand_input_merged = dict_to_2d_array(cluster_dict_seqs_merged)\n",
    "rand_input_all = dict_to_2d_array(cluster_dict_seqs_all)\n",
    "\n",
    "print(rand_input_merged[0][0]) # seqs in row 0, clusters in row 1\n",
    "print(rand_input_merged[0][1])\n",
    "\n",
    "#######\n",
    "\n",
    "# check that both lists have the same gene seqences in the same number\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "same_values = Counter(rand_input_merged.iloc[0].values) == Counter(rand_input_all.iloc[0].values)\n",
    "\n",
    "#print(same_values)\n",
    "\n",
    "c1 = Counter(rand_input_merged.iloc[0].values)\n",
    "c2 = Counter(rand_input_all.iloc[0].values)\n",
    "\n",
    "# Show which values have different counts\n",
    "diffs = {}\n",
    "\n",
    "all_keys = set(c1.keys()).union(c2.keys())\n",
    "for key in all_keys:\n",
    "    if c1[key] != c2[key]:\n",
    "        diffs[key] = {'merged': c1[key], 'all': c2[key]}\n",
    "\n",
    "first_key = next(iter(diffs))\n",
    "print(\"Differences:\", diffs[first_key])\n",
    "print(len(diffs))\n",
    "\n",
    "#########\n",
    "\n",
    "# Get the first rows as arrays\n",
    "merged_row = rand_input_merged.iloc[0].values\n",
    "all_row = rand_input_all.iloc[0].values\n",
    "\n",
    "print(all_row)\n",
    "print(merged_row)\n",
    "\n",
    "# Make a copy of the column names to align\n",
    "merged_columns = list(rand_input_merged.iloc[0].values)\n",
    "used_indices = set()\n",
    "ordered_columns = []\n",
    "\n",
    "#print(f\"merged cols: {merged_columns}\")\n",
    "\n",
    "for val in all_row:\n",
    "    # Find the first unused column index in rand_input_merged where the value matches\n",
    "    for i, v in enumerate(merged_row):\n",
    "        if v == val and i not in used_indices:\n",
    "            ordered_columns.append(merged_columns[i])\n",
    "            used_indices.add(i)\n",
    "            break  # move to the next value in all_row\n",
    "\n",
    "#print(ordered_columns)\n",
    "# Reorder the columns\n",
    "rand_input_merged_new = rand_input_merged[ordered_columns]\n",
    "\n",
    "\n",
    "# Get the order of columns by sorting the values in the first row (row 0)\n",
    "#sorted_columns = rand_input_merged.iloc[0].sort_values().index\n",
    "#print(sorted_columns)\n",
    "\n",
    "# Reorder the DataFrame columns based on that\n",
    "#rand_input_merged = rand_input_merged[sorted_columns]\n",
    "\n",
    "# reorder merged dataset to match order of dna hashes in all dataset\n",
    "#rand_input_merged_new = rand_input_merged.reindex(columns=rand_input_all.columns)\n",
    "\n",
    "#print(f\"rand input new: {rand_input_merged_new}\")\n",
    "\n",
    "# put sorted clusters into Rand index\n",
    "ri = rand_score(rand_input_all.iloc[1], rand_input_merged_new.iloc[1])\n",
    "print(f\"Rand Index: {ri}\")\n",
    "\n",
    "ari = adjusted_rand_score(rand_input_all.iloc[1], rand_input_merged.iloc[1])\n",
    "print(f\"Adjusted Rand Index: {ari}\")\n",
    "\n",
    "# put sorted clusters into mutual information\n",
    "mutual_info = mutual_info_score(rand_input_all.iloc[1], rand_input_merged.iloc[1])\n",
    "print(f\"Mutual Information: {mutual_info}\")\n",
    "\n",
    "adj_mutual_info = adjusted_mutual_info_score(rand_input_all.iloc[1], rand_input_merged.iloc[1])\n",
    "print(f\"Adjusted Mutual Information: {adj_mutual_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2c12c-9032-4dad-bf0c-9f71e0d1b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context search\n",
    "\n",
    "G.neighbors() # from nx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
