import os
from pathlib import Path


configfile: "config.yaml"


# in snakemake profile:
# conda_prefix: ".snakemake/conda"
shell.executable("/bin/bash")

RESULTS = Path(config["results_directory"], "results")


#### LOGGING

from datetime import datetime

LOGDIR = RESULTS / "logs"

def log_path(rule_name, wildcards=None):
    sub = f"{rule_name}"
    if wildcards:
        wc = "_".join(str(getattr(wildcards, k)) for k in sorted(wildcards.keys()))
        return LOGDIR / sub / f"{wc}.log"
    return LOGDIR / f"{rule_name}.log"

####


# later change this rule to look for multiple results (one from each step, ggcaller, panaroo etc)
rule all:
    input:
        RESULTS / "pangenomerge" / "done.txt",


### split isolates into batches (to make running poppunk feasible)


checkpoint split_assemblies_tsv:
    input:
        tsv=config["assemblies_tsv"],
    output:
        outdir=directory(RESULTS / "poppunk" / "batches"),
        index=RESULTS / "poppunk" / "batches" / "batches.index",
    params:
        max_isolates=50,
    resources:
        mem=config["resources_small"]["mem"],
        runtime=config["resources_small"]["runtime"],
        nodes=config["resources_small"]["nodes"],
        slurm_partition=config["resources_small"]["slurm_partition"],
    log:
        lambda wc: log_path("split_assemblies_tsv"),
    shell:
        r"""

        mkdir -p "$(dirname "{log}")"
        exec > >(tee -a "{log}") 2>&1

        echo "[split_assemblies_tsv] start: $(date -Is)"
        echo "[split_assemblies_tsv] input: {input.tsv}"
        echo "[split_assemblies_tsv] outdir: {output.outdir}"
        echo "[split_assemblies_tsv] max_isolates: {params.max_isolates}"

        mkdir -p {output.outdir}

        in="{input.tsv}"
        outdir="{output.outdir}"
        max="{params.max_isolates}"

        # split into blocks of samples
        split -l "{params.max_isolates}" "{input.tsv}" "{output.outdir}"/assemblies_batch_
        
        # rename to assemblies_batch_000.tsv, assemblies_batch_001.tsv, etc.
        n=0
        for f in {output.outdir}/assemblies_batch_*; do
            mv "$f" {output.outdir}/assemblies_batch_$(printf "%03d.tsv" $n)
            n=$((n+1))
        done

        # create and test index
        for f in {output.outdir}/assemblies_batch_*.tsv; do
            bn=$(basename "$f")                 # assemblies_batch_000.tsv
            id=${{bn#assemblies_batch_}}        # 000.tsv
            id=${{id%.tsv}}                     # 000
            echo "$id"
        done > "{output.index}"

        test -s "{output.index}"

        """


def read_ids(path):
    with open(path) as f:
        return [ln.strip() for ln in f if ln.strip()]


def get_batch_ids(wildcards):
    ckpt = checkpoints.split_assemblies_tsv.get(**wildcards)
    return read_ids(ckpt.output.index)


def poppunk_done_files(wildcards):
    batches = get_batch_ids(wildcards)
    return expand(
        str(RESULTS / "poppunk" / "assign" / "{batch}" / "done.txt"), batch=batches
    )


### run poppunk


rule poppunk_assign:
    input:
        assemblies=lambda wc: RESULTS / "poppunk" / "batches" / f"assemblies_batch_{wc.batch}.tsv",
    output:
        outdir=directory(RESULTS / "poppunk" / "assign" / "{batch}"),
        done=RESULTS / "poppunk" / "assign" / "{batch}" / "done.txt",
    params:
        db=config["poppunk_db"],
    conda:
        "envs/poppunk.yaml"
    group:
        "job_array"
    threads: config["resources_batch"]["threads"]
    resources:
        mem=config["resources_batch"]["mem"],
        runtime=config["resources_batch"]["runtime"],
        nodes=config["resources_batch"]["nodes"],
        slurm_partition=config["resources_batch"]["slurm_partition"],
    log:
        lambda wc: log_path("poppunk_assign", wc),
    shell:
        r"""

        mkdir -p "$(dirname "{log}")"
        exec > >(tee -a "{log}") 2>&1

        echo "[poppunk_assign] start: $(date -Is) batch={wildcards.batch}"
        echo "[poppunk_assign] db: {params.db}"
        echo "[poppunk_assign] assemblies: {input.assemblies}"
        echo "[poppunk_assign] outdir: {output.outdir}"
        echo "[poppunk_assign] threads: {threads}"

        mkdir -p {output.outdir}
        poppunk_assign --db {params.db} \
            --query {input.assemblies} \
            --threads {threads} \
            --output {output.outdir} \
            --serial
        echo "ok" > {output.done}
        """


rule poppunk_assign_all:
    input:
        poppunk_done_files,
    output:
        touch(RESULTS / "poppunk" / "assign" / "ALL_DONE.txt"),


### from poppunk, obtain lists of assemblies in each cluster
# and index containing the paths of all these lists


checkpoint poppunk_to_cluster_lists:
    input:
        batches_done=RESULTS / "poppunk" / "assign" / "ALL_DONE.txt",
        assemblies=config["assemblies_tsv"],
    output:
        clusters_dir=directory(RESULTS / "clusters"),
        combined_clusters=RESULTS / "clusters" / "combined_clusters.csv",
        index=RESULTS
        / "clusters"
        / "sizebalanced_clusters_index.csv"
        # also have all clusterfiles as output but not sure how to add that here,
    params:
        poppunk_dir=RESULTS / "poppunk" / "assign",
        min_n=config["min_cluster_size"],
        max_n=config["max_cluster_size"],
    conda:
        "envs/R.yaml"
    threads: config["resources_batch"]["threads"]
    resources:
        mem=config["resources_batch"]["mem"],
        runtime=config["resources_batch"]["runtime"],
        nodes=config["resources_batch"]["nodes"],
        slurm_partition=config["resources_batch"]["slurm_partition"],
    log:
        lambda wc: log_path("poppunk_to_cluster_lists"),
    shell:
        r"""

        mkdir -p "$(dirname "{log}")"
        exec > >(tee -a "{log}") 2>&1

        echo "[poppunk_to_cluster_lists] start: $(date -Is)"
        echo "[poppunk_to_cluster_lists] poppunk_dir: {params.poppunk_dir}"
        echo "[poppunk_to_cluster_lists] assemblies: {input.assemblies}"
        echo "[poppunk_to_cluster_lists] outdir: {output.clusters_dir}"
        echo "[poppunk_to_cluster_lists] min/max: {params.min_n}/{params.max_n}"

        mkdir -p {output.clusters_dir}
        
        ### get list of all poppunk cluster designations

        # first pick the first cluster csv as the header source (from poppunk output dir)
        first="$(ls -1 {params.poppunk_dir}/*/*_clusters.csv | head -n 1)"

        # concatenate all *_clusters.csv with a single header row
        {{ head -n 1 "$first"; for f in {params.poppunk_dir}/*/*_clusters.csv; do tail -n +2 "$f"; done; }} > {output.combined_clusters}

        # create size-balanced clusters (min/max size) + per-cluster lists + index
        Rscript workflow/scripts/adjust_cluster_sizes.R \
            --poppunk-dir {params.poppunk_dir} \
            --assemblies {input.assemblies} \
            --combined-clusters {output.combined_clusters} \
            --min {params.min_n} \
            --max {params.max_n} \
            --outdir {output.clusters_dir}

        # check that sizebalanced_clusters_index.csv was created
        test -s {output.index}

        """


def read_cluster_ids(index_path):
    with open(index_path) as f:
        return [line.strip() for line in f if line.strip()]


def get_cluster_ids_from_ckpt(wildcards):
    ckpt = checkpoints.poppunk_to_cluster_lists.get(**wildcards)
    return read_cluster_ids(ckpt.output.index)


def panaroo_done_files_from_ckpt(wildcards):
    clusters = get_cluster_ids_from_ckpt(wildcards)
    return expand(str(RESULTS / "panaroo" / "{cluster}" / "done.txt"), cluster=clusters)


### run ggcaller on each cluster


rule ggcaller_cluster:
    input:
        index=RESULTS / "clusters" / "sizebalanced_clusters_index.csv",
        cluster_list=lambda wc: RESULTS
        / "clusters"
        / f"sizebalanced_cluster_{wc.cluster}.txt",
    output:
        outdir=directory(RESULTS / "ggcaller" / "{cluster}"),
        done=RESULTS / "ggcaller" / "{cluster}" / "done.txt",
    conda:
        "envs/ggcaller.yaml"
    group:
        "job_array"
    threads: config["resources_batch"]["threads"]
    resources:
        mem=config["resources_batch"]["mem"],
        runtime=config["resources_batch"]["runtime"],
        nodes=config["resources_batch"]["nodes"],
        slurm_partition=config["resources_batch"]["slurm_partition"],
        cpus_per_task=config["resources_batch"]["cpus_per_task"],
        tasks=config["resources_batch"]["tasks"],
    log:
        lambda wc: log_path("ggcaller_cluster", wc),
    shell:
        r"""

        mkdir -p "$(dirname "{log}")"
        exec > >(tee -a "{log}") 2>&1

        echo "[ggcaller_cluster] start: $(date -Is) cluster={wildcards.cluster}"
        echo "[ggcaller_cluster] cluster_list: {input.cluster_list}"
        echo "[ggcaller_cluster] outdir: {output.outdir}"
        echo "[ggcaller_cluster] threads: {threads}"

        mkdir -p {output.outdir}
        python {config["ggcaller_runner"]} \
            --refs {input.cluster_list} \
            --save --gene-finding-only \
            --out {output.outdir} \
            --threads {threads}
        echo "ok" > {output.done}
        """


def get_cluster_ids(wildcards):
    ckpt = checkpoints.poppunk_to_cluster_lists.get(**wildcards)
    index_path = ckpt.output.index
    return read_cluster_ids(index_path)


def ggcaller_done_files(wildcards):
    clusters = get_cluster_ids(wildcards)
    return expand(
        str(RESULTS / "ggcaller" / "{cluster}" / "done.txt"), cluster=clusters
    )


rule ggcaller_all:
    input:
        ggcaller_done_files,
    output:
        touch(RESULTS / "ggcaller" / "ALL_DONE.txt"),


def read_cluster_ids(index_path):
    with open(index_path) as f:
        return [line.strip() for line in f if line.strip()]


### run panaroo on each cluster
rule panaroo_cluster:
    input:
        gg_done=RESULTS / "ggcaller" / "{cluster}" / "done.txt",
    output:
        outdir=directory(RESULTS / "panaroo" / "{cluster}"),
        graph=RESULTS / "panaroo" / "{cluster}" / "final_graph.gml",
        done=RESULTS / "panaroo" / "{cluster}" / "done.txt",
    params:
        indir=str(RESULTS),
    conda:
        "envs/panaroo.yaml"
    group:
        "job_array"
    threads: config["resources_batch"]["threads"]
    resources:
        mem=config["resources_batch"]["mem"],
        runtime=config["resources_batch"]["runtime"],
        nodes=config["resources_batch"]["nodes"],
        slurm_partition=config["resources_batch"]["slurm_partition"],
        cpus_per_task=config["resources_batch"]["cpus_per_task"],
        tasks=config["resources_batch"]["tasks"],
    log:
        lambda wc: log_path("panaroo_cluster", wc),
    shell:
        r"""

        mkdir -p "$(dirname "{log}")"
        exec > >(tee -a "{log}") 2>&1

        echo "[panaroo_cluster] start: $(date -Is) cluster={wildcards.cluster}"
        echo "[panaroo_cluster] indir: {params.indir}"
        echo "[panaroo_cluster] outdir: {output.outdir}"
        echo "[panaroo_cluster] threads: {threads}"

        mkdir -p {output.outdir}
        gffs=$(ls -1 {params.indir}/ggcaller/{wildcards.cluster}/GFF/*.fa.gff)
        panaroo -i $gffs -o {output.outdir} \
            --threads {threads} \
            --clean-mode strict --refind-mode off --remove-invalid-genes
        test -f {output.graph}
        echo "ok" > {output.done}
        """


# run panaroo on all clusters
rule panaroo_all:
    input:
        gg_all_done=RESULTS / "ggcaller" / "ALL_DONE.txt",
        panaroo_done_files=panaroo_done_files_from_ckpt,
    output:
        touch(RESULTS / "panaroo" / "ALL_DONE.txt"),


# list all panaroo graphs
rule list_panaroo_graphs:
    input:
        all_done=RESULTS / "panaroo" / "ALL_DONE.txt",
    output:
        graph_paths=RESULTS / "pangenomes" / "pangenomes.tsv",
    params:
        results_dir=str(RESULTS),
    resources:
        mem=config["resources_small"]["mem"],
        runtime=config["resources_small"]["runtime"],
        nodes=config["resources_small"]["nodes"],
        slurm_partition=config["resources_small"]["slurm_partition"],
    log:
        lambda wc: log_path("list_panaroo_graphs"),
    shell:
        r"""

        mkdir -p "$(dirname "{log}")"
        exec > >(tee -a "{log}") 2>&1

        echo "[list_panaroo_graphs] start: $(date -Is)"
        echo "[list_panaroo_graphs] results_dir: {params.results_dir}"
        echo "[list_panaroo_graphs] output: {output.graph_paths}"

        mkdir -p $(dirname {output.graph_paths})

        >  {output.graph_paths}
        for d in {params.results_dir}/panaroo/*/; 
            do echo "${d%/}" >> {output.graph_paths}; 
        done
        
        """


# run pangenomerge on the complete list of pangenomes
rule merge_pangenomes:
    input:
        graphs=RESULTS / "pangenomes" / "pangenomes.tsv",
    output:
        outdir=directory(RESULTS / "pangenomerge"),
        done=RESULTS / "pangenomerge" / "done.txt",
    params:
        sqlite_cache=config["sqlite_cache"],
    conda:
        "envs/pangenomerge.yaml"
    threads: config["resources_pangenomerge"]["threads"]
    resources:
        mem=config["resources_pangenomerge"]["mem"],
        runtime=config["resources_pangenomerge"]["runtime"],
        nodes=config["resources_pangenomerge"]["nodes"],
        slurm_partition=config["resources_pangenomerge"]["slurm_partition"],
    log:
        lambda wc: log_path("merge_pangenomes"),
    shell:
        r"""

        mkdir -p "$(dirname "{log}")"
        exec > >(tee -a "{log}") 2>&1

        echo "[merge_pangenomes] start: $(date -Is)"
        echo "[merge_pangenomes] graphs: {input.graphs}"
        echo "[merge_pangenomes] outdir: {output.outdir}"
        echo "[merge_pangenomes] threads: {threads}"
        echo "[merge_pangenomes] sqlite_cache: {params.sqlite_cache}"

        mkdir -p {output.outdir}
        python3 {config["pangenomerge_runner"]} \
            --mode run \
            --outdir {output.outdir} \
            --component-graphs {input.graphs} \
            --threads {threads} \
            --sqlite-cache {params.sqlite_cache}
        echo "ok" > {output.done}
        """
