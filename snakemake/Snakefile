import os
from pathlib import Path

configfile: "config.yaml"
conda_prefix: ".snakemake/conda"
shell.executable("/bin/bash")

RESULTS = Path("results")

# later change this rule to look for multiple results (one from each step, ggcaller, panaroo etc)
rule all:
    input:
        RESULTS / "pangenomerge" / "done.txt"

### run poppunk

checkpoint poppunk_assign:
    input:
        assemblies=config["assemblies_tsv"]
    output:
        outdir=directory(RESULTS / "poppunk" / "assign"),
        done=RESULTS / "poppunk" / "assign" / "done.txt"
    params: db=config["poppunk_db"]
    conda: "envs/poppunk.yaml"
    group: "array_job"
    threads: config["resources_batch"]["threads"]
    resources:
        mem=config["resources_batch"]["mem"],
        runtime=config["resources_batch"]["runtime"],
        nodes=config["resources_batch"]["nodes"],
        slurm_partition=config["resources_batch"]["slurm_partition"],
        cpus_per_task=config["resources_batch"]["cpus_per_task"],
        tasks=config["resources_batch"]["tasks"]
    shell:
        r"""
        mkdir -p {output.outdir}
        poppunk_assign --db {params.db} \
            --query {input.assemblies} \
            --threads {threads} \
            --output {output.outdir} \
            --serial
        echo "ok" > {output.done}
        """

### from poppunk, obtain lists of assemblies in each cluster
# and index containing the paths of all these lists

checkpoint poppunk_to_cluster_lists:
    input:
        done=RESULTS / "poppunk" / "assign" / "done.txt",
        poppunk_dir=RESULTS / "poppunk" / "assign",
        assemblies=config["assemblies_tsv"]
    output:
        clusters_dir=directory(RESULTS / "clusters"),
        combined_clusters=RESULTS / "clusters" / "combined_clusters.csv",
        index=RESULTS / "clusters" / "sizebalanced_clusters_index.csv"
        # also have all clusterfiles as output but not sure how to add that here
    params:
        min_n=config["min_cluster_size"],
        max_n=config["max_cluster_size"]
    conda: "envs/R.yaml"
    threads: config["resources_batch"]["threads"]
    resources:
        mem=config["resources_batch"]["mem"],
        runtime=config["resources_batch"]["runtime"],
        nodes=config["resources_batch"]["nodes"],
        slurm_partition=config["resources_batch"]["slurm_partition"]
    shell:
        r"""
        mkdir -p {output.clusters_dir}
        
        ### get list of all poppunk cluster designations

        # first pick the first cluster csv as the header source (from poppunk output dir)
        first="$(ls -1 {input.poppunk_dir}/*_clusters.csv | head -n 1)"

        # concatenate all *_clusters.csv with a single header row
        {{ head -n 1 "$first"; for f in {input.poppunk_dir}/*_clusters.csv; do tail -n +2 "$f"; done; }} > {output.combined_clusters}

        # create size-balanced clusters (min/max size) + per-cluster lists + index
        Rscript workflow/scripts/adjust_cluster_sizes.R \
            --poppunk-dir {input.poppunk_dir} \
            --assemblies {input.assemblies} \
            --min {params.min_n} \
            --max {params.max_n} \
            --outdir {output.clusters_dir}
        """

### run ggcaller on each cluster

rule ggcaller_cluster:
    input:
        index=RESULTS / "clusters" / "sizebalanced_clusters_index.csv",
        cluster_list=lambda wc: RESULTS / "clusters" / f"sizebalanced_cluster_{wc.cluster}.txt"
    output:
        outdir=directory(RESULTS / "ggcaller" / "{cluster}"),
        done=RESULTS / "ggcaller" / "{cluster}" / "done.txt"
    conda: "envs/ggcaller.yaml"
    group: "array_job"
    threads: config["resources_batch"]["threads"]
    resources:
        mem=config["resources_batch"]["mem"],
        runtime=config["resources_batch"]["runtime"],
        nodes=config["resources_batch"]["nodes"],
        slurm_partition=config["resources_batch"]["slurm_partition"],
        cpus_per_task=config["resources_batch"]["cpus_per_task"],
        tasks=config["resources_batch"]["tasks"]
    shell:
        r"""
        mkdir -p {output.outdir}
        python {config["ggcaller_runner"]} \
            --refs {input.cluster_list} \
            --save --gene-finding-only \
            --out {output.outdir} \
            --threads {threads}
        echo "ok" > {output.done}
        """

def get_cluster_ids(wildcards):
    ckpt = checkpoints.poppunk_to_cluster_lists.get(**wildcards)
    index_path = ckpt.output.index
    return read_cluster_ids(index_path)

def ggcaller_done_files(wildcards):
    clusters = get_cluster_ids(wildcards)
    return expand(str(RESULTS / "ggcaller" / "{cluster}" / "done.txt"), cluster=clusters)

rule ggcaller_all:
    input:
        ggcaller_done_files
    output:
        touch(RESULTS / "ggcaller" / "ALL_DONE.txt")

def read_cluster_ids(index_path):
    with open(index_path) as f:
        return [line.strip() for line in f if line.strip()]

def panaroo_done_files(wildcards):
    # ensure upstream checkpoint and clustering ran
    ckpt = checkpoints.poppunk_assign.get(**wildcards)
    index_path = RESULTS / "clusters" / "sizebalanced_clusters_index.csv"

    # make panaroo finishing as "done" dependent on clustering output existing
    if not index_path.exists():
        raise ValueError("Missing sizebalanced_clusters_index.csv")

    clusters = read_cluster_ids(index_path)
    return expand(str(RESULTS / "panaroo" / "{cluster}" / "done.txt"), cluster=clusters)

### run panaroo on each cluster 
rule panaroo_cluster:
    input:
        gg_done=RESULTS / "ggcaller" / "{cluster}" / "done.txt"
    output:
        outdir=directory(RESULTS / "panaroo" / "{cluster}"),
        graph=RESULTS / "panaroo" / "{cluster}" / "final_graph.gml",
        done=RESULTS / "panaroo" / "{cluster}" / "done.txt"
    params:
        indir=str(RESULTS)
    conda: "envs/panaroo.yaml"
    group: "array_job"
    threads: config["resources_batch"]["threads"]
    resources:
        mem=config["resources_batch"]["mem"],
        runtime=config["resources_batch"]["runtime"],
        nodes=config["resources_batch"]["nodes"],
        slurm_partition=config["resources_batch"]["slurm_partition"],
        cpus_per_task=config["resources_batch"]["cpus_per_task"],
        tasks=config["resources_batch"]["tasks"]
    shell:
        r"""
        mkdir -p {output.outdir}
        gffs=$(ls -1 {params.indir}/ggcaller/{wildcards.cluster}/GFF/*.fa.gff)
        panaroo -i $gffs -o {output.outdir} \
            --threads {threads} \
            --clean-mode strict --refind-mode off --remove-invalid-genes
        test -f {output.graph}
        echo "ok" > {output.done}
        """

# run panaroo on all clusters
rule panaroo_all:
    input:
        gg_all_done=RESULTS / "ggcaller" / "ALL_DONE.txt",
        panaroo_done_files=panaroo_done_files
    output:
        touch(RESULTS / "panaroo" / "ALL_DONE.txt")

# list all panaroo graphs
rule list_panaroo_graphs:
    input:
        all_done=RESULTS / "panaroo" / "ALL_DONE.txt"
    output:
        graph_paths=RESULTS / "pangenomes" / "pangenomes.tsv"
    shell:
        r"""
        mkdir -p $(dirname {output.graph_paths})
        > {output.graph_paths}

        for d in {RESULTS}/panaroo/*/; 
            do echo "${d%/}" >> {output.graph_paths}; 
        done
        
        """

# run pangenomerge on the complete list of pangenomes
rule merge_pangenomes:
    input:
        graphs=RESULTS / "pangenomes" / "pangenomes.tsv"
    output:
        outdir=directory(RESULTS / "pangenomerge"),
        done=RESULTS / "pangenomerge" / "done.txt"
    conda: "envs/pangenomerge.yaml"
    threads: config["resources_pangenomerge"]["threads"]
    resources:
        mem=config["resources_pangenomerge"]["mem"],
        runtime=config["resources_pangenomerge"]["runtime"],
        nodes=config["resources_pangenomerge"]["nodes"],
        slurm_partition=config["resources_pangenomerge"]["slurm_partition"]
    params:
        sqlite_cache=config["sqlite_cache"]
    shell:
        r"""
        mkdir -p {output.outdir}
        python3 {config["pangenomerge_runner"]} \
            --mode run \
            --outdir {output.outdir} \
            --component-graphs {input.graphs} \
            --threads {threads} \
            --sqlite-cache {params.sqlite_cache}
        echo "ok" > {output.done}
        """

